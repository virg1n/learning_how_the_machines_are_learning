{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63aaadc-2e8c-40ed-b4a9-9aa91523f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bc5c5c-9bde-46de-b3b9-893f768de00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt').read().split('\\n')\n",
    "chars = {'.':0,}\n",
    "set_chars = set()\n",
    "for i in names:\n",
    "    for j in str(i):\n",
    "        set_chars.add(j)\n",
    "for i, letter in enumerate(sorted(list(set_chars))):\n",
    "    chars[letter] = i + 1\n",
    "keys_chars = list(chars.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f085a31-25cd-4303-a046-513be2e55258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ... e\n",
    "#  ..e m\n",
    "#  .em m\n",
    "#  emm a\n",
    "#  mma .\n",
    "X, Y = [], []\n",
    "prev_lett = 8\n",
    "\n",
    "for name in names:\n",
    "    list_to_loop_of_names = ['.'] + list(name) + ['.']\n",
    "    for letter in range(len(list_to_loop_of_names)-1):\n",
    "        X_here, k = [], 0\n",
    "        while prev_lett-letter-k > 0:\n",
    "            X_here.append(0)\n",
    "            k += 1\n",
    "        while len(X_here) < prev_lett:\n",
    "            # print(letter, len(X_here))\n",
    "            X_here.append(chars[list_to_loop_of_names[letter+len(X_here)-prev_lett+1]])\n",
    "        X.append(X_here)\n",
    "        Y.append(chars[list_to_loop_of_names[letter+len(X_here)-prev_lett+1]])\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bb8b7a-f241-49ca-bc04-dc3ea63197e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(names):\n",
    "    Xs, Ys = [], []\n",
    "    for name in names:\n",
    "        list_to_loop_of_names = ['.'] + list(name) + ['.']\n",
    "        for letter in range(len(list_to_loop_of_names)-1):\n",
    "            X_here, k = [], 0\n",
    "            while prev_lett-letter-k > 0:\n",
    "                X_here.append(0)\n",
    "                k += 1\n",
    "            while len(X_here) < prev_lett:\n",
    "                X_here.append(chars[list_to_loop_of_names[letter+len(X_here)-prev_lett+1]])\n",
    "            Xs.append(X_here)\n",
    "            Ys.append(chars[list_to_loop_of_names[letter+len(X_here)-prev_lett+1]])\n",
    "    return torch.tensor(Xs), torch.tensor(Ys)\n",
    "random.shuffle(names)\n",
    "n1, n2 = int(0.8 * len(names)), int(0.9 * len(names))\n",
    "Xtrain, Ytrain = make_dataset(names[:n1])\n",
    "Xval, Yval = make_dataset(names[n1:n2])\n",
    "Xtest, Ytest = make_dataset(names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4116536e-8fc7-4c3c-a328-1d47ca01cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, inp_neur, out_neur, bias_req=True):\n",
    "        self.weights = torch.randn((inp_neur, out_neur), generator=g) / inp_neur**0.5\n",
    "        if bias_req: self.bias = torch.zeros(out_neur)\n",
    "        else:        self.bias = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias != None: self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def params(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, num_dim):\n",
    "    self.weight = torch.randn((num_embeddings, num_dim), generator=g)\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def params(self):\n",
    "    return [self.weight]\n",
    "\n",
    "class Flatten:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, C, T = x.shape # 4, 8, 20\n",
    "        if self.n == 1:\n",
    "            x = x.view(x.shape[0], -1)\n",
    "        elif self.n == 2:\n",
    "            x = x.view(B, C//self.n, T * self.n)\n",
    "            if x.shape[1] == 1:\n",
    "                x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def params(self):\n",
    "        return []\n",
    "        \n",
    "\n",
    "class batchNormal:\n",
    "    def __init__(self, neur, calc_mean=True, e=0.0001, momentum=0.1):\n",
    "        self.momentum = momentum\n",
    "        self.e = e\n",
    "        self.weights = torch.ones(neur)\n",
    "        self.bias = torch.zeros(neur)\n",
    "        self.training = True\n",
    "\n",
    "        self.mean_mean = torch.zeros(neur)\n",
    "        self.std_mean = torch.ones(neur)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            x_mean_now = x.mean(0, keepdims=True)\n",
    "            x_std_now = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            x_mean_now = self.mean_mean\n",
    "            x_std_now = self.std_mean\n",
    "        xhat = (x - x_mean_now) / torch.sqrt(x_std_now + self.e) # normalize to unit variance\n",
    "        self.out = self.weights * xhat + self.bias\n",
    "        # self.out = self.weights * (x - x_mean_now)/torch.sqrt(x_std_now + self.e) + self.bias\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.mean_mean = (1 - self.momentum) * self.mean_mean + self.momentum * x_mean_now\n",
    "                self.std_mean = (1 - self.momentum) * self.std_mean + self.momentum * x_std_now\n",
    "\n",
    "        return self.out\n",
    "        \n",
    "    def params(self):\n",
    "        return [self.weights, self.bias]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def params(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3b837355-4d85-4c7c-8e94-ce85010f2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTS \n",
    "num_dim = 20\n",
    "num_input = prev_lett * num_dim\n",
    "num_W1 = 100\n",
    "num_W2 = 27\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6651ca7b-f8ae-467e-a0db-6617c4067456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48167\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(2147483647)\n",
    "# C = torch.randn((27, num_dim), generator=g)\n",
    "layers = [\n",
    "    Embedding(27, num_dim),\n",
    "    Flatten(2), Linear(num_dim*2, num_W1), batchNormal(num_W1), Tanh(),\n",
    "    Flatten(2), Linear(num_W1*2, num_W1), batchNormal(num_W1), Tanh(),\n",
    "    Flatten(2), Linear(num_W1*2, num_W1), batchNormal(num_W1), Tanh(),\n",
    "    Linear(num_W1, num_W2),\n",
    "]\n",
    "\n",
    "P = [p for layer in layers for p in layer.params()]\n",
    "print(sum(p.nelement() for p in P))\n",
    "\n",
    "for p in P:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "58b176a6-19c8-4644-8939-55db179ddc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : torch.Size([4, 8, 20])\n",
      "Flatten : torch.Size([4, 4, 40])\n",
      "Linear : torch.Size([4, 4, 100])\n",
      "batchNormal : torch.Size([4, 4, 100])\n",
      "Tanh : torch.Size([4, 4, 100])\n",
      "Flatten : torch.Size([4, 2, 200])\n",
      "Linear : torch.Size([4, 2, 100])\n",
      "batchNormal : torch.Size([4, 2, 100])\n",
      "Tanh : torch.Size([4, 2, 100])\n",
      "Flatten : torch.Size([4, 200])\n",
      "Linear : torch.Size([4, 100])\n",
      "batchNormal : torch.Size([4, 100])\n",
      "Tanh : torch.Size([4, 100])\n",
      "Linear : torch.Size([4, 27])\n"
     ]
    }
   ],
   "source": [
    "epochs = 70000\n",
    "steps, losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    # samples_on_this_epoch = torch.randint(0, len(Xtrain), (batch_size,), generator=g)\n",
    "    # X_on_this_epoch = Xtrain[samples_on_this_epoch]\n",
    "    # Y_on_this_epoch = Ytrain[samples_on_this_epoch]\n",
    "    # print(C[Xtrain[samples_on_this_epoch]].shape[0])\n",
    "    samples_on_this_epoch = torch.randint(0, len(Xtrain), (4,), generator=g)\n",
    "    X_on_this_epoch = Xtrain[samples_on_this_epoch]\n",
    "    Y_on_this_epoch = Ytrain[samples_on_this_epoch]\n",
    "\n",
    "    \n",
    "    for layer in layers:\n",
    "        X_on_this_epoch = layer(X_on_this_epoch)\n",
    "        print(layer.__class__.__name__, \":\", X_on_this_epoch.shape)\n",
    "    loss = F.cross_entropy(X_on_this_epoch, Y_on_this_epoch)\n",
    "    \n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in P:\n",
    "        p.grad = None\n",
    "        \n",
    "    loss.backward()\n",
    "    lr = 0.1 if epoch/epochs < 0.3 else 0.01\n",
    "\n",
    "    for p in P:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    break\n",
    "\n",
    "# 32, 8, 20 -> 32, 4, 40 -> 32, 2, 80 -> 32, 1, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9fd225ba-fd39-43ac-9301-8c5c318eb4bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1000]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m steps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(losses_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(steps, losses_tensor);\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 1000]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "losses_tensor = torch.tensor(losses).view(-1, 1000).mean(1)\n",
    "steps = torch.arange(losses_tensor.shape[-1])\n",
    "plt.plot(steps, losses_tensor);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aa42b0bd-6b97-4105-bdfd-77578152fed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7616, -2.0494,  2.8830,  3.2525, -0.7210, -1.6412],\n",
      "         [-0.1728, -0.0878,  2.7214,  1.0894, -1.1446,  1.3741],\n",
      "         [-2.2995, -2.1356,  3.2366,  0.2302, -5.0941,  0.2465]],\n",
      "\n",
      "        [[ 1.6332, -0.9990, -1.6378,  4.2474,  4.8157, -1.1279],\n",
      "         [ 2.7929, -0.0114,  0.5554,  6.1378,  6.4657,  0.3503],\n",
      "         [-0.1322,  1.2793,  0.2004, -2.2391, -0.9354,  0.3876]]])\n",
      "--------------------------------------------------------\n",
      "tensor([[[-5.1794e-01, -1.7134e-01,  1.1845e+00,  1.9135e+00],\n",
      "         [-2.4575e-01,  1.2168e+00,  6.8924e-01,  2.8752e-01],\n",
      "         [ 1.5141e+00,  7.4868e-01,  1.8650e+00,  5.5012e-01]],\n",
      "\n",
      "        [[-2.1027e+00, -1.3419e+00, -2.9733e-01,  1.5972e-01],\n",
      "         [-3.5426e+00, -1.4324e-01, -3.8205e-01,  5.4973e-01],\n",
      "         [ 5.5576e-01,  5.5068e-01, -4.3454e-01, -1.8795e-03]]])\n",
      "--------------------------------------------------------\n",
      "tensor([[-0.7683,  0.1548, -0.1824, -1.9318, -1.7465, -0.3174],\n",
      "        [ 0.0840,  0.8730,  1.3831, -0.5793, -0.6818,  1.1791],\n",
      "        [-0.5708, -1.6402,  1.0544,  1.9482, -0.9436,  0.2003],\n",
      "        [-0.2451,  0.0643,  0.9284, -0.0810, -0.3265, -0.9620]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = torch.randn(2, 3, 4)\n",
    "b = torch.randn(4, 6)\n",
    "print(a @ b)\n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "print(a)\n",
    "print('--------------------------------------------------------')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3c38598b-ccb5-4a90-bb00-2e33143692b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9465708a-9f8d-4552-ab68-3f604f7f78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def check_loss(dataX, dataY):\n",
    "    losses_checker = []\n",
    "    # for i in range(100):\n",
    "    X_on_this_epoch = dataX\n",
    "    Y_on_this_epoch = dataY\n",
    "    for layer in layers:\n",
    "        X_on_this_epoch = layer(X_on_this_epoch)\n",
    "    \n",
    "    loss = F.cross_entropy(X_on_this_epoch, Y_on_this_epoch)\n",
    "    losses_checker.append(loss.item())\n",
    "    return torch.tensor(losses_checker).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "abfa205a-3e47-4c26-8fd7-0e7b1241b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0634329319000244\n",
      "2.053928852081299\n"
     ]
    }
   ],
   "source": [
    "print(check_loss(Xval, Yval))\n",
    "print(check_loss(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "919b60cc-0272-440e-9a66-4d1fe75c680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evena.', 'zehiel.', 'nyen.', 'deoper.', 'adelsi.', 'lawan.', 'riwent.', 'bannnech.', 'maith.', 'kaiyan.', 'adnalith.', 'andelise.', 'kamir.', 'castida.', 'zayver.']\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "for i in range(15):\n",
    "    inp_letters = [0] * prev_lett\n",
    "    example = []\n",
    "    while True:\n",
    "        lol = torch.tensor(inp_letters)\n",
    "        X_on_this_epoch = lol.view(1, 8)\n",
    "        \n",
    "        for layer in layers:\n",
    "            X_on_this_epoch = layer(X_on_this_epoch)\n",
    "            # print(X_on_this_epoch.shape)\n",
    "        \n",
    "        probs = F.softmax(X_on_this_epoch, dim=1)\n",
    "        letter = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        inp_letters.append(letter)\n",
    "        inp_letters.pop(0)\n",
    "        example.append(keys_chars[letter])\n",
    "        letter = keys_chars[letter]\n",
    "        if letter == '.':\n",
    "            break\n",
    "    examples.append(''.join(example))\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2666c-8a37-40be-a03b-3f8a2842678e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a7f39-a2c4-436f-aae2-83cc594caf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
